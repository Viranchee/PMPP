{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Index in a 2D image\n",
    "\n",
    "\n",
    "```c\n",
    "int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "int plane = blockIdx.z * blockDim.z + threadIdx.z;\n",
    "\n",
    "int matrixSize = blockDim.x * gridDim.x;\n",
    "\n",
    "int offset = plane * matrixSize \n",
    "if (ROW_MAJOR) {\n",
    "    offset += row * blockDim.x + col;\n",
    "} else if (COL_MAJOR) {\n",
    "    offset += col * blockDim.y + row;\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLAS\n",
    "\n",
    "Level 1: Vector-vector operations\n",
    "Level 2: Matrix-vector operations\n",
    "Level 3: Matrix-matrix operations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 2 MatMul kernels\n",
    "In this chapter we implemented a matrix multiplication kernel that has each\n",
    "thread produce one output matrix element. In this question, you will\n",
    "implement different matrix-matrix multiplication kernels and compare them.\n",
    "\n",
    "- Write a kernel that has each thread produce one output matrix row. Fill in the execution configuration parameters for the design.\n",
    "- Write a kernel that has each thread produce one output matrix column. Fill in the execution configuration parameters for the design.\n",
    "- Analyze the pros and cons of each of the two kernel designs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```c\n",
    "// Textbook GEMM, Square matrix\n",
    "// 1 kernel produces 1 output element\n",
    "__global__ void matmulkernel_0(float* M, float* N, float* P, int width) {\n",
    "  int row = threadIdx.y + blockIdx.y * blockDim.y;\n",
    "  int col = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "\n",
    "  if ((row < width) && (col < width)) {\n",
    "    float pVal = 0;\n",
    "      for (int k = 0; k < width; k++)\n",
    "          pVal += M[row * width + k] * N[k * width + col];\n",
    "        P[row * width + col] = pVal;\n",
    "  }\n",
    "}\n",
    "\n",
    "// Host code\n",
    "dim3 threadsPerBlock(16, 16);\n",
    "dim3 blocksPerGrid((n + 15) / 16, (n + 15) / 16);\n",
    "matMulKernel<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, n);\n",
    "\n",
    "// 1 kernel produces 1 output row\n",
    "__global__ void matmulkernel_1(float* M, float* N, float* P, int width) {\n",
    "  int row = threadIdx.y + blockIdx.y * blockDim.y;\n",
    "  if (row < width) {\n",
    "    for (int col = 0; col < width; col++) {\n",
    "      float pVal = 0;\n",
    "      for (int k = 0; k < width; k++)\n",
    "        pVal = M[row * width + k] + N[k * width + col];\n",
    "      P[row * width + col] = pVal;\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "// 1 kernel produces 1 output column\n",
    "__global__ void matmulkernel_2(float* M, float* N, float* P, int width) {\n",
    "  int col = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "\n",
    "  if (col < width) {\n",
    "    for (int row = 0; row < width; row++) {\n",
    "      float pVal = 0;\n",
    "      for (int k = 0; k < width; k++)\n",
    "        pVal = M[row * width + k] + N[k * width + col];\n",
    "      P[row * width + col] = pVal;\n",
    "    }\n",
    "  }\n",
    "}\n",
    "int threadsPerBlock(256);\n",
    "int blocksPerGrid((n + threadsPerBlock - 1) / threadsPerBlock);\n",
    "matMulKernel<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, n);\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MatMul kernel\n",
    "\n",
    "```c\n",
    "__global__ void matmulKernel(float* A /*Out Vector*/, float* B/*In Matrix*/, float* C/*In Vector*/, int width) {\n",
    "  int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "  float sum = 0;\n",
    "  if (i < width) {\n",
    "    for (int j = 0; j < width; j++)\n",
    "      sum += B[i * width + j] * C[j];\n",
    "    A[i] = sum;\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "void matMulHost(float* h_A, float* h_B, float* h_C, int vectorLen) {\n",
    "  float *d_A, *d_C; // vector\n",
    "  float *d_B, // matrix\n",
    "  int vecSize = sizeof(float) * vectorLen;\n",
    "  int matSize = vecSize * vectorLen;\n",
    "  cudaMalloc((void**)&d_A, vecSize);\n",
    "  cudaMalloc((void**)&d_C, vecSize);\n",
    "  cudaMalloc((void**)&d_B, matSize);\n",
    "  cudaMemset(d_A, 0, vecSize);\n",
    "  cudaMemcpy(d_B, h_B, matSize, cudaMemcpyHostToDevice);\n",
    "  cudaMemcpy(d_C, h_C, vecSize, cudaMemcpyHostToDevice);\n",
    "  \n",
    "  dim3 threads(128);\n",
    "  dim3 blocks((vectorLen + threads - 1) / threads);\n",
    "  matmulKernel<<<blocks, threads>>>(d_A, d_B, d_C, n);\n",
    "\n",
    "  cudaFree(d_B);\n",
    "  cudaFree(d_C);\n",
    "  cudaMemcpy(h_A, d_A, vecSize, cudaMemcpyDeviceToHost);\n",
    "  cudaFree(d_A);\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CUDA Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 48640, 95, 45000, 92.51644736842105)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, M = 300, 150\n",
    "T_Y, T_X = 16, 32\n",
    "threadsPerBlock = T_X * T_Y\n",
    "Blocks = ((N-1)//T_Y + 1) * ((M-1)//T_X + 1)\n",
    "threadsInGrids = threadsPerBlock * Blocks\n",
    "threadsUseful = N * M\n",
    "threadsPerBlock, threadsInGrids, Blocks, threadsUseful, threadsUseful/threadsInGrids*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8010, 5020)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W, H = 400, 500\n",
    "row, col = 20, 10\n",
    "\n",
    "rowMajorOffset = row * W + col\n",
    "colMajorOffset = col * H + row\n",
    "\n",
    "rowMajorOffset, colMajorOffset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1008010"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consider a 3D tensor with a width of 400, a height of 500, and a depth of\n",
    "# 300. The tensor is stored as a one-dimensional array in row-major order.\n",
    "# Specify the array index of the tensor element at x5 10, y5 20, and z5 5.\n",
    "\n",
    "W = 400\n",
    "H = 500\n",
    "D = 300\n",
    "\n",
    "x = 10\n",
    "y = 20\n",
    "z = 5\n",
    "\n",
    "rowMajorOffset = (W * H * z) + (y * W) + x\n",
    "rowMajorOffset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymetal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
